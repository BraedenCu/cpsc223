# Time Complexity and Program Efficiency

## Growth Rate

- It is very difficult to compare algos by measuring their execution time
- The growth rate for an algorithm is the rate at which the cost of the algorithm grows as the size of its input grows

## Big O Notation

- Consider linear search. the linear search algorithm compares the key with the elements in the array sequentially until the key is found or the array is exhausted.
- if the key is not in the array, it requires n comparisons for an array of size n. If they key is in the array, it requires n/2 comparisons on average

## How do we find the growth rate? Big O, Big Omega, Big Theta (Asymptotic Complexity)

- Search for the dominant term, i.e if we are observing 5n + 1/2 n^2 + 1/100 n^3, 18n^3 - 12, 40n^3 + logn, the dominant term would be n^3
  - n dominates for all n larger than some integer
- ***Big O*** : After a certain point, that function grows at least as fast as your runtime function. f(x) does **not** grow more quickly than BigO. **Upper Bound**
  - for example, Big O of n^3 means that at some point your runtime functions runs at least as fast as n^3
- ***Big Omega*** : If there exists constants d>0, n1>0 such that for all n >= n1 : f(n) >= dn^3 then we say f(n) = omega(n^3).f(x) grows **atleast** as quickly as Omega. **lower bound**
- ***Big Theta*** : both **big O** and **big Omega** are equal. The function grows no faster than **big o** but no slower than **big omega**
  - All functions covered in our course will be **big theta** bound, because there will be a tight, well-defined bound for the functions we are writing and looking at in this class
  - **big theta** bounds essentially indicate that we have found a precise (somewhat) way to determine the time complexity of a function
- Runtime vs Time Complexity
  - Running time is a property of an algorithm. It is the maximum number of steps the algorithm can run for, as a function of the length of the input. Time complexity is a property of a computational problem. It is, essentially, the running time of the fastest possible algorithm for that problem

## Best, Worst, and Average Cases

- An input that results in the shortest execution time is called the best-case input and an input that results in the longest execution time is called the worst-case input
- Best-case and worst-case are not representative, but the worst-case analysis is very useful
  - Because you can show that the algorithm will never be slower than the worst-case
- An average case analysis attempts to determine the average amount of time among all possible inputs of the same size. Average case analysis is ideal but difficult to perform because it is hard to determine the relative probabilities and distributions of various inputs
- Worst-case analysis is typically easier to obtain and is thus common, **analysis is generally conducted for the worst case**

## Asymptotic Complexity, AKA BigO Analysis

- O(1) : Constant Time --> Great
- O(logn) : Logrithmic Time --> Nice
- O(n) : Linear Time --> Ok
- O(nlogn) 
- O(n^2) : Exponential Time --> Decent
- O(2^n)


## logrithmic Time

- An algorithm with O(logn) time complexity is called a logrithmic algorithm. The base of the log is 2, but the base does not effect a logrithmic growth rate, so it can be omitted
- The logrithmic algorithm grows slowly as the problem size increases. If you square the input size, you only double the time for the algorithm.    
  - Logrithmic algoritms are very good
  - ANY Logorithm is bounded above with a poloynomial
  